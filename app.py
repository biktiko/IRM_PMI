import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
from tempfile import gettempdir
import hashlib, os
from src.data_loader import read_excel_all, build_weights_dict, questions_from_weights
from src.scoring import long_from_base, join_weights, aggregate_scores, extract_10pt_rating
import src.scoring as scoring
from src.utils import _parse_visit_date, pick_col, brand_theme, brand_bar_chart, _normalize_store_col
import altair as alt
from src import scoring
# NEW ‚Äî –¥–ª—è –∞—É–¥–∏–æ/Supabase
import uuid, mimetypes
from datetime import datetime
from src.supabase import _sb_client, _sb_public_url, _sb_upload, _sb_list_all, _sb_delete, get_latest_excel
from src.audio_ui import render_audio_tab

def _sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

# --- Credentials loader (supports multiple secret/env names) ---
def _load_creds():
    def _get(key):
        # probuyem secrets, inacze env
        try:
            return st.secrets[key]
        except Exception:
            return os.getenv(key)
    username = (
        _get("APP_USERNAME")
        or _get("USERNAME_ENV")
        or "PMI2025"
    )
    password_plain = (
        _get("APP_PASSWORD")
        or _get("PASS_ENV")
        or _get("PASS_HASH_ENV")
    )
    password_hash = _get("APP_PASS_HASH")

    # ete hesh@ chi sahmanvats, bayc password_plain@ 64 heqs tzeq@ hanelu ‚Äî hamarvum enq hesh e
    if not password_hash and password_plain and len(password_plain) == 64 and all(c in "0123456789abcdef" for c in password_plain.lower()):
        password_hash = password_plain
        password_plain = None
    return username, password_plain, password_hash

APP_USERNAME, APP_PASSWORD_PLAIN, APP_PASSWORD_HASH = _load_creds()

def _check_password(inp: str) -> bool:
    # 1) ete ka hesh’ù stugum enq SHA256-ov
    if APP_PASSWORD_HASH:
        return _sha256(inp) == APP_PASSWORD_HASH
    # 2) fallback plain
    if APP_PASSWORD_PLAIN:
        return inp == APP_PASSWORD_PLAIN
    return False

st.set_page_config(page_title="’é’°’≥’°’º÷Ñ’´ ’Ø’•’ø’•÷Ä’´ ’£’∂’°’∞’°’ø’¥’°’∂ ’§’°’∑’¢’∏÷Ä’§", layout="wide")

# --- AUTH ---
if st.sidebar.button("Logout", disabled=not st.session_state.get("auth_ok")):
    for k in ("auth_ok","auth_user"):
        st.session_state.pop(k, None)
    st.rerun()

if not st.session_state.get("auth_ok"):
    st.title("’Ñ’∏÷Ç’ø÷Ñ ’∞’°’¥’°’Ø’°÷Ä’£")
    with st.form("login_form"):
        u = st.text_input("Login", value="")
        p = st.text_input("Password", type="password", value="")
        ok = st.form_submit_button("Login")
        if ok:
            if u == APP_USERNAME and _check_password(p):
                st.session_state["auth_ok"] = True
                st.session_state["auth_user"] = u
                st.success("’Ñ’∏÷Ç’ø÷Ñ’® ’∞’°’ª’∏’≤ ’ß")
                st.rerun()
            else:
                st.error("’ç’≠’°’¨ login ’Ø’°’¥ password")
    st.stop()
# --- –∫–æ–Ω–µ—Ü –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, –Ω–∏–∂–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ---

@st.cache_data(show_spinner=False)
def load_excel_bytes(b: bytes):
    return b

@st.cache_data(show_spinner=True)
def parse_workbooks(path: Path):
    return read_excel_all(path) 

# ===== persistence: save uploads to ./imports and autoload latest =====
IMPORTS_DIR = Path(gettempdir()) / "pmi_uploads"
IMPORTS_DIR.mkdir(parents=True, exist_ok=True)

def _list_saved_excels():
    return sorted(IMPORTS_DIR.glob("*.xls*"), key=lambda p: p.stat().st_mtime, reverse=True)

def _save_uploaded(upl):
    dst = IMPORTS_DIR / upl.name
    dst.write_bytes(upl.getbuffer())
    return dst

st.sidebar.title("’è’æ’µ’°’¨’∂’•÷Ä’´ ’∂’•÷Ä’¥’∏÷Ç’Æ’∏÷Ç’¥")
# upl = st.sidebar.file_uploader("‘≤’•’º’∂’•÷Ñ Excel ÷Ü’°’µ’¨’®", type=["xlsx","xls"])

state = st.session_state
if "bases" not in state:
    state.bases = {}
    state.weights = pd.DataFrame()
    state.last_file = None

alt.themes.register("brand", brand_theme)
alt.themes.enable("brand")

# --- NEW: Supabase Auto-Load ---
DATA_BUCKET = st.secrets.get("SUPABASE_DATA_BUCKET", "data")  # bucket name
sb_client, _ = _sb_client()

if st.sidebar.button("üîÑ ‘π’°÷Ä’¥’°÷Å’∂’•’¨ (Supabase)"):
    st.cache_data.clear()
    st.rerun()

@st.cache_data(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Supabase...", ttl=3600)
def load_remote_excel():
    if not sb_client:
        return None, None
    return get_latest_excel(sb_client, DATA_BUCKET)

remote_content, remote_name = load_remote_excel()
selected_path = None

if remote_content:
    st.sidebar.success(f"‘±’Ø’ø’´’æ ÷Ü’°’µ’¨: {remote_name}")
    # Save to temp
    tmp_path = IMPORTS_DIR / remote_name
    tmp_path.write_bytes(remote_content)
    
    # Parse
    bases_raw, weights_raw = parse_workbooks(tmp_path)
    state.bases = bases_raw
    state.weights = build_weights_dict(weights_raw)
    state.last_file = str(tmp_path)
    selected_path = tmp_path
else:
    # Fallback: check local cache
    existing = _list_saved_excels()
    if existing:
        selected_path = existing[0]
        st.sidebar.warning(f"Supabase-’∏÷Ç’¥ ÷Ü’°’µ’¨ ’π’Ø’°, ÷Ö’£’ø’°’£’∏÷Ä’Æ’æ’∏÷Ç’¥ ’ß: {selected_path.name}")
    else:
        st.sidebar.error(f"Supabase: ÷Ü’°’µ’¨ ’π’´ ’£’ø’∂’æ’•’¨ '{DATA_BUCKET}' ’¢’°’™’∂’∏÷Ç’¥")

# –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω —Ñ–∞–π–ª –∏ –æ–Ω –∏–∑–º–µ–Ω–∏–ª—Å—è ‚Äî –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å –±–∞–∑—ã –∏ –≤–µ—Å–∞
if selected_path is not None and state.get("last_file") != str(selected_path):
    bases_raw, weights_raw = read_excel_all(str(selected_path))
    state.bases = bases_raw
    state.weights = build_weights_dict(weights_raw)
    state.last_file = str(selected_path)

# –ù–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ–≥–æ–Ω–µ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏ –∑–∞–Ω–æ–≤–æ (–±–µ–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≤ session)
state.ratings_list = []

if not state.bases:
    st.info("‘≤’•’º’∂’•÷Ñ Excel ÷Ü’°’µ’¨’®")
    st.stop()
# ===== end persistence block =====

st.sidebar.markdown("---")

prepared = []
state.ratings_list = []

with st.container():  # —Å–∫—Ä—ã—Ç—ã–π ’∫’°’ø÷Ä’°’Ω’ø’∏’≤’°’Ø’°’∂ ’¢’¨’∏’Ø ’°’º’°’∂÷Å UI
    for scen, base in state.bases.items():
        # comment: normalize scenario name
        scen = scen if scen in ["BR1", "BR2", "BR3", "BR4", "BR5"] else "BR1"

        # comment: filter base by scenario
        df_scene = scoring.filter_by_scenario_column(base, scen)

        # comment: small header (no huge H2)
        # st.markdown(f"<h4>{scen} ’Ω÷Å’•’∂’°÷Ä’´ ’¥’∑’°’Ø’∏÷Ç’¥’®</h4>", unsafe_allow_html=True)

        # comment: get question keys from weights
        qkeys = questions_from_weights(state.weights, scen)
        if not qkeys:
            st.warning("‘ø’∑’´’º’∂’•÷Ä’´ ’©’•÷Ä’©’∏÷Ç’¥ ’ø’æ’µ’°’¨ ’Ω÷Å’•’∂’°÷Ä’´ ’∞’°÷Ä÷Å’•÷Ä ’π’•’∂ ’∞’°’µ’ø’∂’°’¢’•÷Ä’æ’•’¨÷â ‘Ω’∂’§÷Ä’∏÷Ç’¥ ’•’∂÷Ñ ’Ω’ø’∏÷Ç’£’•’¨ LAS/YAP ’©’•÷Ä’©’•÷Ä’®÷â")
            continue

        # comment: choose store column & skip columns
        options_cols = list(df_scene.columns)
        # –∞–≤—Ç–æ-’∫’°÷Ä’¶’•’¨ ’≠’°’∂’∏÷Ç’©’´ ’Ω’µ’∏÷Ç’∂’°’Ø’® (–±–µ–∑ UI)
        store_col = (
            pick_col(df_scene, keys=["store","’Ñ’°’Ω’∂’°’≥’µ’∏÷Ç’≤’´ ’°’∂’æ’°’∂’∏÷Ç’¥","‘Ω’°’∂’∏÷Ç’©","‘Ω’°’∂’∏÷Ç’©’´ ’°’∂’æ’°’∂’∏÷Ç’¥","Shop","Store"])
            or (options_cols[1] if len(options_cols) > 1 else options_cols[0])
        )

        # ’Ü’à’ê’Ñ‘±‘º‘ª‘∂‘±’ë‘ª‘± ’Ü‘±‘∂’é‘±’Ü‘ª’Ö ’Ñ‘±‘≥‘±‘∂‘ª’Ü’à’é (—á—Ç–æ–±—ã '‘Ω’°’∂’∏÷Ç’©  A' == '‘Ω’°’∂’∏÷Ç’© A')
        if store_col in df_scene.columns:
            df_scene[store_col] = _normalize_store_col(df_scene[store_col])

        # —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ ‚Äî –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–±–µ–∑ UI)
        skip_cols = [c for c in options_cols[:8] if c != store_col]

        # comment: prepare long form for scoring
        drop_list = [c for c in skip_cols if c in df_scene.columns and c != store_col]
        df_for_long = df_scene.drop(columns=drop_list, errors="ignore")

        long_df = long_from_base(
            df_for_long,
            store_col=store_col,
            non_question_cols=skip_cols,
            scenario=scen,
            qkeys_norm=qkeys,
        )

        # comment: collect ratings per scenario
        ratings_scene = extract_10pt_rating(df_scene, scen, store_col)
        state.ratings_list.append(ratings_scene)

        # comment: join weights and accumulate
        merged = join_weights(long_df, state.weights, scen)
        prepared.append(merged)

    # --- 2) Build df_all and ratings, validate ---
    df_all = pd.concat(prepared, ignore_index=True) if prepared else pd.DataFrame()
    ratings = pd.concat(state.ratings_list, ignore_index=True) if state.ratings_list else pd.DataFrame()

    # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π (–µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø—Ä–æ—Å–∫–æ—á–∏–ª–æ)
    if not df_all.empty and "store" in df_all.columns:
        df_all["store"] = _normalize_store_col(df_all["store"])

    if df_all.empty:
        st.error("’Ä’∂’°÷Ä’°’æ’∏÷Ä ’π’•’≤’°’æ ’∫’°’ø÷Ä’°’Ω’ø’•’¨ ’ø’æ’µ’°’¨’∂’•÷Ä’®÷â ’ç’ø’∏÷Ç’£’•÷Ñ ’∂’•÷Ä’¢’•’º’æ’°’Æ ÷Ü’°’µ’¨’® ÷á ’Ø’∑’´’º’∂’•÷Ä’´ ’©’•÷Ä’©’•÷Ä’®÷â")
        st.stop()

    # === Visits & comments extraction (robust) ===
    def _to_minutes(series: pd.Series) -> pd.Series:
        # datetime -> –º–∏–Ω—É—Ç—ã
        if pd.api.types.is_datetime64_any_dtype(series):
            return series.dt.hour * 60 + series.dt.minute + series.dt.second / 60.0
        # timedelta -> –º–∏–Ω—É—Ç—ã
        if pd.api.types.is_timedelta64_dtype(series):
            return series.dt.total_seconds() / 60.0
        # numeric (Excel time as fraction of day)
        if pd.api.types.is_numeric_dtype(series):
            return series.astype(float) * 24.0 * 60.0
        # strings: ’∂’∏÷Ä’¥’°’¨’´’¶’∏÷Ç’¥ ’•’∂÷Ñ ’∞’°’µ’Ø’°’Ø’°’∂ ’•÷Ä’Ø’Ø’•’ø "÷â"
        s = series.astype(str).str.strip()
        s = s.replace({"": np.nan, "None": np.nan})
        s = s.str.replace("\u0589", ":", regex=False)  # Armenian :
        s = s.str.replace(".", ":", regex=False)       # ’•÷Ä’¢’•’¥’∂ ’Ø’•’ø’•÷Ä
        # –∏–∑–≤–ª–µ—á—å HH:MM
        ext = s.str.extract(r'^\s*(?P<h>\d{1,2})[:](?P<m>\d{2})(?::(?P<s>\d{2}))?\s*$')
        h = pd.to_numeric(ext["h"], errors="coerce")
        m = pd.to_numeric(ext["m"], errors="coerce")
        sec = pd.to_numeric(ext["s"], errors="coerce").fillna(0)
        return h * 60 + m + sec / 60.0

    visits_rows = []
    comments_rows = []

    for scen_name, base_df in state.bases.items():
        dfb = base_df.copy()
        store_col = pick_col(dfb, keys=["store","’Ñ’°’Ω’∂’°’≥’µ’∏÷Ç’≤’´ ’°’∂’æ’°’∂’∏÷Ç’¥","‘Ω’°’∂’∏÷Ç’©","‘Ω’°’∂’∏÷Ç’©’´ ’°’∂’æ’°’∂’∏÷Ç’¥","Shop","Store"]) or dfb.columns[0]
        if store_col in dfb.columns:
            dfb[store_col] = _normalize_store_col(dfb[store_col])
        start_col = pick_col(dfb, keys=["‘±’µ÷Å’•’¨’∏÷Ç’©’µ’°’∂ ’Ω’Ø’´’¶’¢","Visit start"]) or pick_col(dfb, contains=["’Ω’Ø’´’¶","start"])
        end_col   = pick_col(dfb, keys=["‘±’µ÷Å’•’¨’∏÷Ç’©’µ’°’∂ ’°’æ’°÷Ä’ø","Visit end"])   or pick_col(dfb, contains=["’°’æ’°÷Ä’ø","end"])
        dur_col   = pick_col(dfb, keys=["‘±’µ÷Å’´ ’®’∂’§’∞’°’∂’∏÷Ç÷Ä ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂","Duration"]) or pick_col(dfb, contains=["’ø÷á’∏’≤","dur"])
        date_col  = pick_col(dfb, keys=["‘±’µ÷Å’•’¨’∏÷Ç’©’µ’°’∂ ’°’¥’Ω’°’©’´’æ","Visit date"]) or pick_col(dfb, contains=["’°’¥’Ω’°’©","date"])

        # ’Ü’∏÷Ä’¥’°’¨’´’¶’°÷Å’æ’°’Æ ’°’¥’Ω’°’©’æ’´ Series (’•’©’• ’π’Ø’°, ’¢’∏’¨’∏÷Ä ’°÷Ä’™’•÷Ñ’∂’•÷Ä’® NaT)
        if date_col and date_col in dfb.columns:
            base_date = _parse_visit_date(dfb[date_col])
        else:
            base_date = pd.Series(pd.NaT, index=dfb.index)

        has_time = bool(start_col or end_col or dur_col)
        if has_time:
            start_min = _to_minutes(dfb.get(start_col, pd.Series(index=dfb.index, dtype="float")))
            end_min   = _to_minutes(dfb.get(end_col,   pd.Series(index=dfb.index, dtype="float")))
            dur_min   = _to_minutes(dfb.get(dur_col,   pd.Series(index=dfb.index, dtype="float")))

            # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ start/end
            calc_min = end_min - start_min
            calc_min = calc_min.mask(calc_min < 0, calc_min + 24*60)  # –ø–µ—Ä–µ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—á—å

            # –ò—Ç–æ–≥–æ–≤–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ’•’©’• ’Ø’° ’°’º’°’∂’±’´’∂ dur_min ‚Äî ’æ’•÷Ä÷Å’∂’∏÷Ç’¥ ’•’∂÷Ñ ’°’µ’∂, ’°’µ’¨’°’∫’•’Ω calc_min
            visit_duration_min = dur_min.where(dur_min.notna(), calc_min)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ ’¥’´’°’µ’∂ ’•’©’• ’Ø’° ’∏’π ’§’°’ø’°÷Ä’Ø ’°’¥’Ω’°’©’´’æ
            if base_date.notna().any():
                visit_start = base_date + pd.to_timedelta(start_min, unit="m")
                visit_end   = base_date + pd.to_timedelta(end_min,   unit="m")
                cross_mid = (end_min.notna() & start_min.notna() & (end_min < start_min))
                visit_end = visit_end.where(~cross_mid, visit_end + pd.Timedelta(days=1))
            else:
                visit_start = pd.Series(pd.NaT, index=dfb.index)
                visit_end   = pd.Series(pd.NaT, index=dfb.index)

            tmp = pd.DataFrame({
                "store": dfb[store_col],
                "scenario": scen_name if scen_name in ["BR1","BR2","BR3","BR4","BR5"] else "BR1",
                "visit_start": visit_start,
                "visit_end": visit_end,
                "visit_duration_min": visit_duration_min
            })
            tmp = tmp.dropna(subset=["store"])  # —É–±—Ä–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ ’°’º’°’∂÷Å ’≠’°’∂’∏÷Ç’©’´
            if tmp["visit_start"].notna().any():
                tmp["hour"] = tmp["visit_start"].dt.hour
                def _tod(h):
                    if pd.isna(h): return "Unknown"
                    h = int(h)
                    if 6 <= h < 12:  return "Morning"
                    if 12 <= h < 18: return "Day"
                    if 18 <= h < 22: return "Evening"
                    return "Night"
                tmp["time_of_day"] = tmp["hour"].map(_tod)
            else:
                tmp["time_of_day"] = "Unknown"
            visits_rows.append(tmp)

        # –û—Ç–∫—Ä—ã—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
        com1_col = pick_col(dfb, keys=["’Ñ’•’Ø’∂’°’¢’°’∂’∏÷Ç’©’µ’∏÷Ç’∂"]) or pick_col(dfb, contains=["’¥’•’Ø’∂"])
        # –ù–µ—Å–∫–æ–ª—å–∫–æ ’ø’°÷Ä’¢’•÷Ä’°’Ø’∂’•÷Ä’´ ’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’∞’°÷Ä÷Å’´ ’¢’°÷Ä’•’¨’°’æ’∏÷Ç’¥’∂’•÷Ä’´ ’¥’°’Ω’´’∂
        com2_col = (
            pick_col(dfb, keys=["‘ª’û’∂’π ’Ø’°÷Ä’•’¨’´ ’ß ’°’∂’•’¨ ÷É’∏÷Ä’±’® ’¢’°÷Ä’•’¨’°’æ’•’¨’∏÷Ç ’∞’°’¥’°÷Ä", "‘ª’û’∂’π ’Ø’°÷Ä’•’¨’´ ’ß ’°’∂’•’¨ ÷É’∏÷Ä’±’® ’¢’°÷Ä’•’¨’°’æ’•’¨’∏÷Ç ’∞’°’¥’°÷Ä÷â"])
            or pick_col(dfb, contains=["’¢’°÷Ä’•’¨’°’æ","’´’∂’π"])
        )
        for c_name, c_col in [("Comment", com1_col), ("Improvement", com2_col)]:
            if c_col and c_col in dfb.columns:
                cc = dfb[[store_col, c_col]].copy()
                cc.columns = ["store","text"]
                cc["scenario"] = scen_name if scen_name in ["BR1","BR2","BR3","BR4","BR5"] else "BR1"
                cc["type"] = c_name
                cc = cc.dropna(subset=["text"])
                # —É–±—Ä–∞—Ç—å ‘±’µ’∏/’à’π –∏ –ø—É—Å—Ç—è–∫–∏
                cc["text"] = cc["text"].astype(str).str.strip()
                cc = cc[cc["text"].str.len() >= 5]
                cc = cc[~cc["text"].isin(["‘±’µ’∏","’à’π","Yes","No"])]
                comments_rows.append(cc)

    visits_df = pd.concat(visits_rows, ignore_index=True) if visits_rows else pd.DataFrame()
    comments_df = pd.concat(comments_rows, ignore_index=True) if comments_rows else pd.DataFrame()

# --- 3) Filters AFTER df_all exists ---
with st.expander("’ñ’´’¨’ø÷Ä’•÷Ä", expanded=False):
    st.header("’ñ’´’¨’ø÷Ä’•÷Ä")
    stores = sorted(df_all["store"].dropna().unique().tolist())
    scenarios = sorted(df_all["scenario"].dropna().unique().tolist())
    sections = sorted(df_all["section"].dropna().unique().tolist())

    sel_scen = st.multiselect("’ç÷Å’•’∂’°÷Ä’∂’•÷Ä", options=scenarios, default=scenarios)
    sel_stores = st.multiselect("‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä", options=stores, default=stores)
    sel_sec = st.multiselect("‘≤’°’™’´’∂’∂’•÷Ä", options=sections, default=sections)

# --- 4) Apply filters once selections are made ---
flt = df_all[
    df_all["store"].isin(sel_stores)
    & df_all["scenario"].isin(sel_scen)
    & df_all["section"].isin(sel_sec)
]

with st.expander("’Ü’•÷Ä’§÷Ä’æ’°’Æ ’ø’æ’µ’°’¨’∂’•÷Ä"):

    total_records = len(flt)
    stores = flt['store'].unique()
    scenarios = flt['scenario'].nunique()
    sections = flt['section'].nunique()

    st.markdown(f"""
    **‘≥÷Ä’°’º’∏÷Ç’¥’∂’•÷Ä:** {total_records:,}  
    **‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä ({len(stores)}):**  
    {', '.join(stores)}  
    **’ç÷Å’•’∂’°÷Ä’∂’•÷Ä:** {scenarios}  
    **‘≤’°’™’´’∂’∂’•÷Ä:** {sections}
    """, unsafe_allow_html=True)

from src import ui
pq, ps, psc, pstore, _raw = aggregate_scores(flt)

# --- weights per question (merge from raw) ---
if not _raw.empty and not pq.empty:
    qw = (
        _raw.groupby(["store","scenario","section","question_key"], as_index=False)
            .agg(
                weight_in_section=("weight_in_section","first"),
                weight_in_scenario=("weight_in_scenario","first"),
                # NEW: –Ω—É–∂–µ–Ω –¥–ª—è ‘±’µ’∏/’à’π –∏ —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ KeyError
                answer_bin=("answer_bin","first")
            )
    )
    pq = pq.merge(qw, on=["store","scenario","section","question_key"], how="left")

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –¥–æ–ª—è–º –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∫–∞–∑–∞
    for c in ["weight_in_section","weight_in_scenario"]:
        if c in pq.columns:
            pq[c] = pd.to_numeric(pq[c], errors="coerce")

    # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Å –≤–æ–ø—Ä–æ—Å–∞ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç ‚Äî –±–µ—Ä—ë–º –≤–µ—Å –≤ —Å–µ–∫—Ü–∏–∏)
    w_frac = pq["weight_in_scenario"].fillna(pq["weight_in_section"])
    pq["question_weight_pct"] = (w_frac * 100).round(2)

    # –≤–∫–ª–∞–¥ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∏—Ç–æ–≥ —Å—Ü–µ–Ω–∞—Ä–∏—è
    pq["weighted_score_pct"] = (pq["score_question_pct"] * w_frac).round(2)

tab_overview, tab_stores, tab_scen, tab_sections, tab_compare, tab_visits, tab_audio = st.tabs(
    ["‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä", "‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä", "’ç÷Å’•’∂’°÷Ä’∂’•÷Ä", "‘≤’°’™’´’∂’∂’•÷Ä", "’Ä’°’¥’•’¥’°’ø’•’¨", "‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä", "‘±’∏÷Ç’§’´’∏"]
)

with tab_overview:
    total_scores = scoring.total_score_table(flt)

    if not total_scores.empty:
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ’ç’é‘µ’ê’Ä’à’í
        with st.expander("‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä ’æ’´’≥’°’Ø’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂", expanded=True):
            stores_cnt = int(total_scores["store"].nunique())
            scen_cnt = int(flt["scenario"].nunique())
            avg_pct = float(total_scores["total_score_pct"].mean() * 100)
            med_pct = float(total_scores["total_score_pct"].median() * 100)
            best_row = total_scores.iloc[total_scores["total_score_pct"].idxmax()]
            worst_row = total_scores.iloc[total_scores["total_score_pct"].idxmin()]

            # 1-—è —Å—Ç—Ä–æ–∫–∞ –º–µ—Ç—Ä–∏–∫
            r1c1, r1c2, r1c3, r1c4 = st.columns(4)
            r1c1.metric("‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä", stores_cnt)
            r1c2.metric("’Ñ’´’ª’´’∂ %", f"{avg_pct:.1f}%")
            r1c3.metric("’Ñ’•’§’´’°’∂ %", f"{med_pct:.1f}%")
            r1c4.metric("’ç÷Å’•’∂’°÷Ä’∂’•÷Ä", scen_cnt)

            # 2-—è —Å—Ç—Ä–æ–∫–∞ (–¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–º–µ—â–∞—é—Ç—Å—è –ª—É—á—à–µ)
            r2c1, r2c2 = st.columns(2)
            r2c1.metric("‘º’°’æ’°’£’∏÷Ç’µ’∂ ’≠’°’∂’∏÷Ç’©",
                        f"{best_row['store']} ‚Äî {best_row['total_score_pct']*100:.1f}%")
            r2c2.metric("’é’°’ø’°’£’∏÷Ç’µ’∂ ’≠’°’∂’∏÷Ç’©",
                        f"{worst_row['store']} ‚Äî {worst_row['total_score_pct']*100:.1f}%")

            scen_avgs = scoring.scenario_score_table(flt)
            if not scen_avgs.empty:
                scen_mean = (
                    scen_avgs.groupby("scenario", as_index=False)
                             .agg(avg_pct=("scenario_score_pct", "mean"))
                             .assign(avg_pct=lambda d: d["avg_pct"].round(1))
                             .rename(columns={"scenario": "’ç÷Å’•’∂’°÷Ä", "avg_pct": "’Ñ’´’ª’´’∂ %"})
                             .sort_values("’ç÷Å’•’∂’°÷Ä")
                )
                st.markdown("**’Ñ’´’ª’´’∂ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’® ’®’Ω’ø ’Ω÷Å’•’∂’°÷Ä’´ (%)**")
                # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
                ch = alt.Chart(scen_mean).mark_bar().encode(
                    x=alt.X("’ç÷Å’•’∂’°÷Ä:N", sort=None, title="’ç÷Å’•’∂’°÷Ä"),
                    y=alt.Y("’Ñ’´’ª’´’∂ %:Q", title="’Ñ’´’ª’´’∂ %", scale=alt.Scale(domain=[0, 100])),
                    tooltip=["’ç÷Å’•’∂’°÷Ä", alt.Tooltip("’Ñ’´’ª’´’∂ %:Q", format=".1f")]
                ).properties(height=320)
                txt = ch.mark_text(baseline="bottom", dy=-4).encode(
                    text=alt.Text("’Ñ’´’ª’´’∂ %:Q", format=".1f")
                )
                st.altair_chart(ch + txt, use_container_width=True)

        # –î–∞–ª–µ–µ –æ–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥
        st.markdown("#### ‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä’´ ’®’∂’§’∞’°’∂’∏÷Ç÷Ä ’æ’°÷Ä’Ø’°’∂’´’∑ (Total Score)")
        st.caption(scoring.total_score_caption_hy())
        scores_df = total_scores.rename(columns={"total_score_pct_display": "score"})[["store", "score"]]
        ui.rating_table(scores_df, "score", "‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä ")
    else:
        st.info("’è’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂ ’®’∂’§’∞’°’∂’∏÷Ç÷Ä ’æ’°÷Ä’Ø’°’∂’´’∑’´ ’∞’°’¥’°÷Ä÷â")

    st.divider()

    # # --- 2) Heatmap second ---
    # st.markdown("#### ‘≤’°’™’´’∂’∂’•÷Ä’´ ’ª’•÷Ä’¥’°÷Ñ’°÷Ä’ø’•’¶")
    # ui.heatmap_sections(ps)

    # st.divider()

    # --- 3) Table last ---
    if not ratings.empty:
        # Compute once, show clean table
        avg = (
            ratings
            .groupby(["store", "scenario"], as_index=False)["rating_10pt"]
            .mean()
            .round(2)
            .rename(columns={"rating_10pt": "avg_rating_10pt"})
            .sort_values(["avg_rating_10pt", "store"], ascending=[False, True])
            .reset_index(drop=True)
        )

        st.markdown("#### ‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä’´ ’ø’∫’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® (1‚Äì10)")
        st.dataframe(avg, use_container_width=True)
    else:
        st.info("’è’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂ ÷Å’∏÷Ç÷Å’°’§÷Ä’•’¨’∏÷Ç ’∞’°’¥’°÷Ä")


with tab_stores:
    st.subheader("‘Ω’°’∂’∏÷Ç’©’´ ’∫÷Ä’∏÷Ü’´’¨")
    st.caption(scoring.caption_store_profile_hy())

    # –¢–æ–ª—å–∫–æ –≤—ã–±–æ—Ä –º–∞–≥–∞–∑–∏–Ω–∞ —Å–Ω–∞—á–∞–ª–∞
    store_options = sorted(df_all["store"].dropna().unique()) if "store" in df_all.columns else []
    if not store_options:
        st.info("‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä ’π’Ø’°’∂ ÷Å’∏÷Ç÷Å’°’§÷Ä’•’¨’∏÷Ç ’∞’°’¥’°÷Ä÷â")
    else:
        sel_store = st.selectbox("‘∏’∂’ø÷Ä’•÷Ñ ’≠’°’∂’∏÷Ç’©", options=store_options, key="store_profile_store")

        # –°—Ü–µ–Ω–∞—Ä–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–∞–≥–∞–∑–∏–Ω–∞ (–≤—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å—Ä–∞–∑—É)
        scen_scores_full = scoring.scenario_score_table(df_all)
        scen_scores_store = scen_scores_full[scen_scores_full["store"] == sel_store].copy()
        scen_scores_store = scen_scores_store.rename(columns={"scenario":"’ç÷Å’•’∂’°÷Ä", "scenario_score_pct":"‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %"})

        st.markdown("**’ç÷Å’•’∂’°÷Ä’∂’•÷Ä’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä’® (% ’∞’∂’°÷Ä’°’æ’∏÷Ä ’°’º’°’æ’•’¨’°’£’∏÷Ç’µ’∂’´÷Å)**")
        if scen_scores_store.empty:
            st.warning("’è’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂ ’®’∂’ø÷Ä’æ’°’Æ ’≠’°’∂’∏÷Ç’©’´ ’∞’°’¥’°÷Ä÷â")
        else:
            height = max(160, 26 * len(scen_scores_store))
            ch1 = alt.Chart(scen_scores_store).mark_bar(size=22).encode(
                y=alt.Y("’ç÷Å’•’∂’°÷Ä:N", sort='-x', title="’ç÷Å’•’∂’°÷Ä"),
                x=alt.X("‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", title="‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %", scale=alt.Scale(domain=[0, 100])),
                tooltip=["’ç÷Å’•’∂’°÷Ä", alt.Tooltip("‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", format=".0f")]
            ).properties(height=height)
            tx1 = ch1.mark_text(align="left", dx=4).encode(text=alt.Text("‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", format=".0f"))
            st.altair_chart(ch1 + tx1, use_container_width=True)
            st.dataframe(scen_scores_store.sort_values("’ç÷Å’•’∂’°÷Ä"), use_container_width=True)

        st.divider()

        # –¢–µ–ø–µ—Ä—å –≤—ã–±–æ—Ä –æ–¥–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        scen_options_store = sorted(scen_scores_store["’ç÷Å’•’∂’°÷Ä"].unique()) if not scen_scores_store.empty else []
        if not scen_options_store:
            st.info("’ç÷Å’•’∂’°÷Ä’∂’•÷Ä ’π’Ø’°’∂ ’ø’æ’µ’°’¨ ’≠’°’∂’∏÷Ç’©’´ ’∞’°’¥’°÷Ä÷â")
        else:
            sel_scen = st.selectbox("‘∏’∂’ø÷Ä’•÷Ñ ’Ω÷Å’•’∂’°÷Ä ’¥’°’∂÷Ä’°’¥’°’Ω’∂ ’ø’•’Ω’∂’•’¨’∏÷Ç ’∞’°’¥’°÷Ä", options=scen_options_store, key="store_profile_scen")

            prof = scoring.store_profile_breakdown(df_all, sel_store, sel_scen)
            if not prof.empty:
                # –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫
                prof = scoring.apply_section_order(prof, "section")
                sec_summary = (
                    prof.groupby("section", as_index=False)
                        .agg(section_score_pct=("section_score_pct","first"))
                        .rename(columns={"section":"‘≤’°’™’´’∂", "section_score_pct":"‘≤’°’™’∂’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ %"})
                )

                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                sec_summary["‘≤’°’™’´’∂"] = pd.Categorical(
                    sec_summary["‘≤’°’™’´’∂"],
                    categories=scoring.SECTION_ORDER,
                    ordered=True
                )
                sec_summary = sec_summary.sort_values("‘≤’°’™’´’∂")

                st.markdown(f"**‘≤’°’™’´’∂’∂’•÷Ä’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä’® ({sel_scen})**")
                height2 = max(160, 26 * len(sec_summary))
                ch2 = alt.Chart(sec_summary).mark_bar(size=22).encode(
                    y=alt.Y("‘≤’°’™’´’∂:N", sort='-x', title="‘≤’°’™’´’∂"),
                    x=alt.X("‘≤’°’™’∂’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", title="‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %", scale=alt.Scale(domain=[0, 100])),
                    tooltip=["‘≤’°’™’´’∂", alt.Tooltip("‘≤’°’™’∂’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", format=".0f")]
                ).properties(height=height2)
                tx2 = ch2.mark_text(align="left", dx=4).encode(text=alt.Text("‘≤’°’™’∂’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", format=".0f"))
                st.altair_chart(ch2 + tx2, use_container_width=True)
                st.dataframe(sec_summary, use_container_width=True)

                st.markdown(f"**‘∏’Ω’ø ’∞’°÷Ä÷Å’•÷Ä’´ ({sel_scen})**")
                q_cols = prof[["section","question_key","answer","weight_share_pct","earned_pct"]].rename(columns={
                    "section":"‘≤’°’™’´’∂",
                    "question_key":"’Ä’°÷Ä÷Å",
                    "answer":"’ä’°’ø’°’Ω’≠’°’∂",
                    "weight_share_pct":"’î’°’∑’´ ’¢’°’™’´’∂’® %",
                    "earned_pct":"’ç’ø’°÷Å’æ’°’Æ %"
                }).sort_values(["‘≤’°’™’´’∂","’Ä’°÷Ä÷Å"])
                st.dataframe(q_cols, use_container_width=True)

                scen_pct = prof["scenario_score_pct"].iloc[0]
                col_m, col_p = st.columns([1,3])
                with col_m:
                    st.metric(label=f"‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’® ({sel_scen})", value=f"{scen_pct:.2f}%")
                with col_p:
                    st.progress(min(max(float(scen_pct)/100.0, 0.0), 1.0))

with tab_scen:
    st.subheader("’å’•’µ’ø’´’∂’£ ’®’Ω’ø ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’´")
    st.caption(scoring.caption_scenario_page_hy())

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–ª–æ–±–∞–ª—å–Ω–æ –¥–∞–Ω–Ω—ã–µ flt
    scen_list = sorted(flt["scenario"].dropna().unique()) if "scenario" in flt.columns else []
    if not scen_list:
        st.info("’ç÷Å’•’∂’°÷Ä’∂’•÷Ä ’π’Ø’°’∂÷â")
    else:
        sel_scen = st.selectbox("’ç÷Å’•’∂’°÷Ä", options=scen_list, key="scen_main")

        # –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è
        scen_df = flt[flt["scenario"] == sel_scen].copy()
        scen_df["w"] = pd.to_numeric(scen_df.get("weight_in_scenario"), errors="coerce").fillna(0.0)
        scen_df = scen_df[scen_df["w"] > 0]
        if "question_key" in scen_df.columns:
            scen_df = scen_df[~scoring._opinion_mask_from_key(scen_df["question_key"])]

        section_opts = sorted(scen_df["section"].dropna().unique()) if "section" in scen_df.columns else []
        # –ü–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–¥–µ–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        section_opts = sorted(
            [s for s in section_opts if s in scoring.SECTION_ORDER],
            key=lambda x: scoring.SECTION_ORDER.index(x)
        ) + [s for s in section_opts if s not in scoring.SECTION_ORDER]
        section_opts_ui = ["‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®"] + section_opts
        sel_section = st.selectbox("‘≤’°’™’´’∂ (’¥’´’∂’π÷á ’¥’•’Ø)", options=section_opts_ui, key="scen_section")

        # –í–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª
        if sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
            qdf = scen_df[scen_df["section"] == sel_section]
            question_opts = sorted(qdf["question_key"].dropna().unique()) if "question_key" in qdf.columns else []
            question_opts_ui = ["‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®"] + question_opts
            sel_question = st.selectbox("’Ä’°÷Ä÷Å (’¥’´’∂’π÷á ’¥’•’Ø)", options=question_opts_ui, key="scen_question")
        else:
            sel_question = "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®"

        # –†–∞—Å—á–µ—Ç
        if sel_question != "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®":
            scores_subset = scoring.scenario_subset_scores(flt, sel_scen, question=sel_question)
            label = f"’Ä’°÷Ä÷Å’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ % ({sel_question})"
        elif sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
            scores_subset = scoring.scenario_subset_scores(flt, sel_scen, section=sel_section)
            label = f"‘≤’°’™’∂’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ % ({sel_section})"
        else:
            scores_subset = scoring.scenario_subset_scores(flt, sel_scen)
            label = "’ç÷Å’•’∂’°÷Ä’´ ’®’∂’§’∞’°’∂’∏÷Ç÷Ä %"

        if scores_subset.empty:
            st.warning("’è’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂ ’®’∂’ø÷Ä’æ’°’Æ ÷Ü’´’¨’ø÷Ä’´ ’∞’°’¥’°÷Ä÷â")
        else:
            # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–π—Ç–∏–Ω–≥–∞
            tbl = scores_subset.rename(columns={"store":"store","value_pct":label}).sort_values(label, ascending=False)
            ui.rating_table(tbl.rename(columns={label:"score"}), "score", label)

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
            ch = alt.Chart(tbl).mark_bar().encode(
                x=alt.X("store:N", sort="-y", title="‘Ω’°’∂’∏÷Ç’©"),
                y=alt.Y(f"{label}:Q", title="%", scale=alt.Scale(domain=[0,100])),
                tooltip=["store", alt.Tooltip(label, format=".1f")]
            ).properties(height=360)
            tx = ch.mark_text(baseline="bottom", dy=-4).encode(text=alt.Text(label, format=".0f"))
            st.altair_chart(ch + tx, use_container_width=True)

            # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω —Ä–∞–∑–¥–µ–ª –∏–ª–∏ –≤–æ–ø—Ä–æ—Å)
            if sel_question != "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®" or sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
                st.markdown("**’Ñ’°’∂÷Ä’°’¥’°’Ω’∂’•÷Ä ’®’Ω’ø ’≠’°’∂’∏÷Ç’©’´ (’°’Ω’ø’´’≥’°’∂’°’æ’∏÷Ä’æ’°’Æ ’®’Ω’ø –ø—Ä–æ—Ü’•’∂’ø’°)**")
                st.dataframe(tbl, use_container_width=True)

with tab_sections:
    st.subheader("‘≤’°’™’´’∂’∂’•÷Ä’´ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä")
    st.caption(scoring.caption_sections_page_hy())

    # === –§–∏–ª—å—Ç—Ä—ã ===
    scen_opts = sorted(state.weights["scenario"].dropna().unique()) if not state.weights.empty else []
    sel_scenarios = st.multiselect(
        "’ç÷Å’•’∂’°÷Ä’∂’•÷Ä",
        options=scen_opts,
        default=scen_opts,
        key="sec_scen_multi"
    )

    store_opts = sorted(df_all["store"].dropna().unique()) if not df_all.empty else []
    sel_section_stores = st.multiselect(
        "‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä",
        options=store_opts,
        default=store_opts,
        key="sec_store_multi"
    )

    # === –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞ (–ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏–µ–º/–º–∞–≥–∞–∑–∏–Ω–∞–º) ===
    sec_scores = scoring.section_scores(flt, scenarios=sel_scenarios, stores=sel_section_stores)
    if sec_scores.empty:
        st.info("’â’Ø’°’∂ ’ø’æ’µ’°’¨’∂’•÷Ä ’®’∂’ø÷Ä’æ’°’Æ ÷Ü’´’¨’ø÷Ä’•÷Ä’∏’æ÷â")
    else:
        sec_scores = scoring.apply_section_order(sec_scores, "section")
        tbl = (
            sec_scores
            .rename(columns={
                "store":"‘Ω’°’∂’∏÷Ç’©",
                "scenario":"’ç÷Å’•’∂’°÷Ä",
                "section":"‘≤’°’™’´’∂",
                "section_pct":"‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %"
            })
        )
        tbl["‘≤’°’™’´’∂"] = pd.Categorical(tbl["‘≤’°’™’´’∂"], categories=scoring.SECTION_ORDER, ordered=True)
        tbl = tbl.sort_values(["‘≤’°’™’´’∂","’ç÷Å’•’∂’°÷Ä","‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %"], ascending=[True, True, False])
        st.dataframe(tbl, use_container_width=True)

    # –¢–∞–±–ª–∏—Ü–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ (—Ç–∞–∫–∂–µ –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø–æ—Ä—è–¥–æ–∫)
    if not pq.empty and sel_scenarios and sel_section_stores:
        pq_filtered = pq[pq["scenario"].isin(sel_scenarios) & pq["store"].isin(sel_section_stores)].copy()
        if "section" in pq_filtered.columns:
            pq_filtered = scoring.apply_section_order(pq_filtered, "section")
            pq_filtered["section"] = pd.Categorical(pq_filtered["section"], categories=scoring.SECTION_ORDER, ordered=True)
        # EXCLUDE: ¬´MS-’´ ’®’∂’§’∞’°’∂’∏÷Ç÷Ä ’ø’∫’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä¬ª –∏ ¬´‘ª’û’∂’π ’Ø’°÷Ä’•’¨’´ ’ß ’°’∂’•’¨ ÷É’∏÷Ä’±’® ’¢’°÷Ä’•’¨’°’æ’•’¨’∏÷Ç ’∞’°’¥’°÷Ä¬ª (–∏ –∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–∏)
        if "question_key" in pq_filtered.columns:
            mask_opinion = scoring._opinion_mask_from_key(pq_filtered["question_key"])
            pq_filtered = pq_filtered[~mask_opinion]

        if not pq_filtered.empty:
            # ‘±’µ’∏/’à’π: —Å–ø–µ—Ä–≤–∞ –ø—Ä–æ–±—É–µ–º answer_bin, –∏–Ω–∞—á–µ –±–µ—Ä—ë–º score_question_pct (0/1)
            ans = pd.to_numeric(pq_filtered.get("answer_bin"), errors="coerce")
            if "score_question_pct" in pq_filtered.columns:
                ans = ans.fillna(pd.to_numeric(pq_filtered["score_question_pct"], errors="coerce"))
            pq_filtered["ans_label"] = np.where(
                ans >= 1, "‘±’µ’∏",
                np.where(ans == 0, "’à’π", "")
            )

            st.markdown("**’Ä’°÷Ä÷Å’•÷Ä’´ ’∫’°’ø’°’Ω’≠’°’∂’∂’•÷Ä’®**")
            st.dataframe(
                pq_filtered.sort_values(["scenario","section","store","question_key"])[
                    ["store","scenario","section","question_key","ans_label"]
                ]
                .rename(columns={
                    "store":"‘Ω’°’∂’∏÷Ç’©",
                    "scenario":"’ç÷Å’•’∂’°÷Ä",
                    "section":"‘≤’°’™’´’∂",
                    "question_key":"’Ä’°÷Ä÷Å",
                    "ans_label":"’ä’°’ø’°’Ω’≠’°’∂"
                }),
                use_container_width=True
            )
        else:
            st.info("’â’Ø’°’∂ ’∞’°÷Ä÷Å’•÷Ä ’ø’æ’µ’°’¨ ÷Ü’´’¨’ø÷Ä’•÷Ä’∏’æ÷â")

with tab_compare:
    st.subheader("’Ä’°’¥’•’¥’°’ø’•’¨ ’•÷Ä’Ø’∏÷Ç ’≠’°’∂’∏÷Ç’©")
    st.caption(scoring.caption_compare_page_hy())

    stores = sorted(flt["store"].dropna().unique()) if not flt.empty else []
    scens  = sorted(flt["scenario"].dropna().unique()) if not flt.empty else []

    if len(stores) < 2:
        st.info("’Ä’°’¥’•’¥’°’ø’•’¨’∏÷Ç ’∞’°’¥’°÷Ä ’°’∂’∞÷Ä’°’™’•’∑’ø ’•’∂ ’°’º’∂’æ’°’¶’∂ ’•÷Ä’Ø’∏÷Ç ’≠’°’∂’∏÷Ç’©÷â")
    else:
        col_a, col_b, col_s = st.columns([1,1,1.2])
        with col_a:
            store_a = st.selectbox("‘Ω’°’∂’∏÷Ç’© A", options=stores, key="cmp_store_a")
        with col_b:
            store_b = st.selectbox("‘Ω’°’∂’∏÷Ç’© B", options=[s for s in stores if s != store_a], key="cmp_store_b")
        with col_s:
            scen = st.selectbox("’ç÷Å’•’∂’°÷Ä", options=["‘≤’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’®"] + scens, key="cmp_scen")

        sel_section = "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®"
        sel_question = "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®"
        if scen != "‘≤’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’®":
            base = flt[flt["scenario"] == scen].copy()
            base["w"] = pd.to_numeric(base.get("weight_in_scenario"), errors="coerce").fillna(0.0)
            base = base[base["w"] > 0]
            if "question_key" in base.columns:
                base = base[~scoring._opinion_mask_from_key(base["question_key"])]
            sect_opts = ["‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®"] + sorted(base["section"].dropna().unique()) if "section" in base.columns else ["‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®"]
            sel_section = st.selectbox("‘≤’°’™’´’∂", options=sect_opts, key="cmp_section")
            if sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
                qdf = base[base["section"] == sel_section]
                q_opts = ["‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®"] + sorted(qdf["question_key"].dropna().unique()) if "question_key" in qdf.columns else ["‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®"]
                sel_question = st.selectbox("’Ä’°÷Ä÷Å", options=q_opts, key="cmp_question")

        if scen == "‘≤’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’®":
            ts = scoring.total_score_table(flt)
            pair = ts[ts["store"].isin([store_a, store_b])].copy()
            pair["value_pct"] = (pair["total_score_pct"] * 100).round(2)
            label = "‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä % (’¢’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä)"
        else:
            kw = {}
            if sel_question != "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®":
                kw["question"] = sel_question
            elif sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
                kw["section"] = sel_section
            pair = scoring.scenario_subset_scores(flt, scen, **kw)
            pair = pair[pair["store"].isin([store_a, store_b])].copy()
            label = (
                f"’ç÷Å’•’∂’°÷Ä’´ % ({scen})" if not kw else
                (f"‘≤’°’™’∂’´ % ({sel_section})" if "section" in kw else f"’Ä’°÷Ä÷Å’´ % ({sel_question})")
            )

        if pair.empty or pair["store"].nunique() < 2:
            st.warning("’è’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂ ’®’∂’ø÷Ä’æ’°’Æ ÷Ü’´’¨’ø÷Ä’•÷Ä’∏’æ÷â")
        else:
            va = float(pair.loc[pair["store"] == store_a, "value_pct"].iloc[0])
            vb = float(pair.loc[pair["store"] == store_b, "value_pct"].iloc[0])
            mcol1, mcol2 = st.columns(2)
            mcol1.metric(f"{store_a}", f"{va:.2f}%")
            mcol2.metric(f"{store_b}", f"{vb:.2f}%")

            chart_df = pair.rename(columns={"store":"‘Ω’°’∂’∏÷Ç’©","value_pct":label})
            ch = alt.Chart(chart_df).mark_bar(size=60).encode(
                x=alt.X("‘Ω’°’∂’∏÷Ç’©:N", sort=None),
                y=alt.Y(f"{label}:Q", scale=alt.Scale(domain=[0,100]), title="%"),
                color="‘Ω’°’∂’∏÷Ç’©:N",
                tooltip=["‘Ω’°’∂’∏÷Ç’©", alt.Tooltip(label, format=".2f")]
            ).properties(height=320)
            tx = ch.mark_text(baseline="bottom", dy=-4).encode(text=alt.Text(label, format=".0f"))
            st.altair_chart(ch + tx, use_container_width=True)

            if scen != "‘≤’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’®" and sel_question == "‘≤’∏’¨’∏÷Ä ’∞’°÷Ä÷Å’•÷Ä’®":
                sec = scoring.section_scores(flt, scenarios=[scen], stores=[store_a, store_b])
                if sel_section != "‘≤’∏’¨’∏÷Ä ’¢’°’™’´’∂’∂’•÷Ä’®":
                    sec = sec[sec["section"] == sel_section]
                if not sec.empty:
                    sec = scoring.apply_section_order(sec.copy())
                    sec = sec.rename(columns={"store":"‘Ω’°’∂’∏÷Ç’©","section":"‘≤’°’™’´’∂","section_pct":"‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %"})
                    ch2 = alt.Chart(sec).mark_bar().encode(
                        x=alt.X("‘≤’°’™’´’∂:N", title="‘≤’°’™’´’∂"),
                        y=alt.Y("‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", scale=alt.Scale(domain=[0,100]), title="%"),
                        color="‘Ω’°’∂’∏÷Ç’©:N",
                        tooltip=["‘Ω’°’∂’∏÷Ç’©","‘≤’°’™’´’∂", alt.Tooltip("‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ %:Q", format=".1f")]
                    ).properties(height=360)
                    st.altair_chart(ch2, use_container_width=True)

# --- NEW: Visits tab restored ---
with tab_visits:
    st.subheader("‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä")
    st.caption("‘±’µ’Ω’ø’•’≤ ’∂’•÷Ä’Ø’°’µ’°÷Å’æ’°’Æ ’•’∂ ’°’µ÷Å’•÷Ä’´ ÷Ñ’°’∂’°’Ø’®, ’¥’´’ª’´’∂ ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’®, ’™’°’¥’°’∂’°’Ø’°’µ’´’∂ ’¢’°’∑’≠’æ’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’® "
               "÷á ’°’µ÷Å’•’¨’∏’≤’∂’•÷Ä’´ ’¥’•’Ø’∂’°’¢’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®÷â ’ñ’´’¨’ø÷Ä’•÷Ä’® ’Ø’´÷Ä’°’º’æ’∏÷Ç’¥ ’•’∂ ’¥’´’°’µ’∂ ’°’µ’Ω ’¢’°’™’∂’∏÷Ç’¥÷â")

    if visits_df is None or visits_df.empty:
        st.info("‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’ø’æ’µ’°’¨’∂’•÷Ä ’π’Ø’°’∂÷â")
    else:
        # CLEAN: —É–±—Ä–∞—Ç—å 'nan'/–ø—É—Å—Ç—ã–µ –º–∞–≥–∞–∑–∏–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ
        vbase = visits_df.copy()
        s_clean = vbase["store"].astype(str).str.strip()
        vbase = vbase[s_clean.notna() & (s_clean != "") & (s_clean.str.lower() != "nan")].copy()

        # –§–∏–ª—å—Ç—Ä—ã (—Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        v_stores = sorted(vbase["store"].dropna().unique().tolist())
        v_scens = sorted(vbase["scenario"].dropna().unique().tolist())
        with st.expander("’ñ’´’¨’ø÷Ä’•÷Ä (‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä)", expanded=True):
            f_scens = st.multiselect("’ç÷Å’•’∂’°÷Ä’∂’•÷Ä", options=v_scens, default=v_scens, key="vis_scen")
            f_stores = st.multiselect("‘Ω’°’∂’∏÷Ç’©’∂’•÷Ä", options=v_stores, default=v_stores, key="vis_store")
            max_dur = st.slider("’Ñ’°’Ø’Ω. ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂ (÷Ä’∏’∫’•) ‚Äî ’∞’•’º’°÷Å’∂’•’¨ ’Æ’°’µ÷Ä’°’∞’•’≤ ’°÷Ä’™’•÷Ñ’∂’•÷Ä’®",
                                min_value=30, max_value=180, value=60, step=5)

        vflt = vbase[vbase["scenario"].isin(f_scens) & vbase["store"].isin(f_stores)].copy()
        vflt["visit_duration_min"] = pd.to_numeric(vflt["visit_duration_min"], errors="coerce")

        # FILTER: –∏—Å–∫–ª—é—á–∏—Ç—å —è–≤–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        before_len = len(vflt)
        vflt = vflt[(vflt["visit_duration_min"].isna()) | ((vflt["visit_duration_min"] >= 0) & (vflt["visit_duration_min"] <= max_dur))]
        removed_outliers = before_len - len(vflt)
        if removed_outliers > 0:
            st.caption(f"’ë’∏÷Ç÷Å’°’§÷Ä’¥’°’∂ ’∞’°’¥’°÷Ä ’∞’•’º’°÷Å’æ’•’¨ ’ß {removed_outliers} ’°’µ÷Å, ’∏÷Ä’∏’∂÷Å ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’® > {max_dur} ÷Ä’∏’∫’• (’∞’°’æ’°’∂’°’¢’°÷Ä ’Ω’≠’°’¨ ’£÷Ä’°’º’∏÷Ç’¥).")

        # T–æ–ø-–º–µ—Ç—Ä–∏–∫–∏
        total_visits = int(len(vflt))
        avg_dur = float(vflt["visit_duration_min"].mean()) if total_visits else 0.0
        c1, c2 = st.columns(2)
        c1.metric("‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ÷Ñ’°’∂’°’Ø", total_visits)
        c2.metric("’Ñ’´’ª’´’∂ ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂ (÷Ä’∏’∫’•)", f"{avg_dur:.0f}")

        # 1) ’Ñ’´’ª’´’∂ ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂ ÷á ’°’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’©’æ’°÷Ñ’°’∂’°’Ø
        st.markdown("### ’Ñ’´’ª’´’∂ ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂ ÷á ’°’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’©’æ’°÷Ñ’°’∂’°’Ø")
        agg = (
            vflt.groupby(["store","scenario"], as_index=False)
                .agg(visits=("store","count"), avg_duration_min=("visit_duration_min","mean"))
        )
        agg["avg_duration_min"] = agg["avg_duration_min"].round(0).astype("Int64")
        st.dataframe(agg.sort_values(["store","scenario"]), use_container_width=True)

        # 2) ‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® ’®’Ω’ø ’≠’°’∂’∏÷Ç’©’∂’•÷Ä’´ (’¢’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’∏’æ)
        st.markdown("### ‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® ’®’Ω’ø ’≠’°’∂’∏÷Ç’©’∂’•÷Ä’´ (’¢’∏’¨’∏÷Ä ’Ω÷Å’•’∂’°÷Ä’∂’•÷Ä’∏’æ)")
        by_store = vflt.groupby("store", as_index=False).size().rename(columns={"size":"visits"})
        st.dataframe(by_store.sort_values("visits", ascending=False), use_container_width=True)

        h = max(240, 24 * len(by_store))
        ch_store = alt.Chart(by_store).mark_bar(size=20).encode(
            y=alt.Y("store:N", sort='-x', title="‘Ω’°’∂’∏÷Ç’©"),
            x=alt.X("visits:Q", title="‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ÷Ñ’°’∂’°’Ø"),
            tooltip=["store","visits"]
        ).properties(height=h)
        st.altair_chart(ch_store, use_container_width=True)

        # 3) ‘±’µ÷Å’•’¨’∏÷Ç’¥’∂’•÷Ä’® ’®’Ω’ø ÷Ö÷Ä’æ’° ’¥’°’Ω’•÷Ä’´
        st.markdown("### ‘±’µ÷Å’•’¨’∏÷Ç’¥’∂’•÷Ä’® ’®’Ω’ø ÷Ö÷Ä’æ’° ’¥’°’Ω’•÷Ä’´")
        tod = (
            vflt.groupby(["time_of_day","scenario"], as_index=False)
                .size().rename(columns={"size":"visits"})
        )
        order_tod = ["Morning","Day","Evening","Night","Unknown"]
        tod["time_of_day"] = pd.Categorical(tod["time_of_day"], categories=order_tod, ordered=True)
        st.dataframe(
            tod.pivot_table(index="time_of_day", columns="scenario", values="visits", fill_value=0),
            use_container_width=True
        )

        ch_tod = alt.Chart(tod).mark_bar().encode(
            x=alt.X("time_of_day:N", title="’ï÷Ä’æ’° ’¥’°’Ω"),
            y=alt.Y("visits:Q", title="‘±’µ÷Å’•’¨’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä"),
            color=alt.Color("scenario:N", title="’ç÷Å’•’∂’°÷Ä"),
            tooltip=["time_of_day","scenario","visits"]
        ).properties(height=320)
        st.altair_chart(ch_tod, use_container_width=True)

        # 4) ‘±’µ÷Å’•’¨’∏÷Ç’¥’∂’•÷Ä’´ ’™’°’¥’•÷Ä’∂ ’∏÷Ç ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®
        st.markdown("### ‘±’µ÷Å’•’¨’∏÷Ç’¥’∂’•÷Ä’´ ’™’°’¥’•÷Ä’∂ ’∏÷Ç ’ø÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®")
        cols = ["store","scenario","visit_start","visit_end","visit_duration_min","time_of_day"]
        st.dataframe(vflt[cols].sort_values(["store","scenario","visit_start"]), use_container_width=True)

        if vflt["visit_start"].notna().any():
            ch_scatter = alt.Chart(vflt).mark_circle(size=80, opacity=0.7).encode(
                x=alt.X("visit_start:T", title="’ç’Ø’´’¶’¢"),
                y=alt.Y("visit_duration_min:Q", title="’è÷á’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂ (÷Ä’∏’∫’•)", scale=alt.Scale(domain=[0, max_dur])),
                color=alt.Color("scenario:N", title="’ç÷Å’•’∂’°÷Ä"),
                tooltip=["store","scenario","visit_start","visit_end",
                         alt.Tooltip("visit_duration_min:Q", format=".0f")]
            ).properties(height=320)
            st.altair_chart(ch_scatter, use_container_width=True)

    # 5) ‘±’µ÷Å’•’¨’∏’≤’∂’•÷Ä’´ ’¥’•’Ø’∂’°’¢’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®
    st.markdown("### ‘±’µ÷Å’•’¨’∏’≤’∂’•÷Ä’´ ’¥’•’Ø’∂’°’¢’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®")
    if comments_df is None or comments_df.empty:
        st.info("’Ñ’•’Ø’∂’°’¢’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ’π’Ø’°’∂÷â")
    else:
        cflt = comments_df.copy()
        s_clean_c = cflt["store"].astype(str).str.strip()
        cflt = cflt[s_clean_c.notna() & (s_clean_c != "") & (s_clean_c.str.lower() != "nan")]
        if 'scenario' in cflt and 'store' in cflt:
            cflt = cflt[cflt["scenario"].isin(f_scens) & cflt["store"].isin(f_stores)]
        st.dataframe(cflt.sort_values(["store","scenario"]), use_container_width=True)

with st.expander("’è’æ’µ’°’¨’∂’•÷Ä’´ ’¢’°’¶’°", expanded=False):
    if not df_all.empty:
        dbg_store = st.selectbox("‘Ω’°’∂’∏÷Ç’© (Debug)", options=sorted(df_all["store"].unique()))
        dbg_scen  = st.selectbox("’ç÷Å’•’∂’°÷Ä (Debug)", options=sorted(df_all["scenario"].unique()))
        from src.scoring import diagnostics_breakdown, diagnostics_summary
        dbg_rows = diagnostics_breakdown(df_all, dbg_store, dbg_scen)
        dbg_sum  = diagnostics_summary(df_all, dbg_store, dbg_scen)

        if dbg_rows.empty:
            st.warning("’â’Ø’°’∂ ’ø’æ’µ’°’¨’∂’•÷Ä ’®’∂’ø÷Ä’æ’°’Æ ’≠’°’∂’∏÷Ç’©’´ / ’Ω÷Å’•’∂’°÷Ä’´ ’∞’°’¥’°÷Ä÷â")
        else:
            st.markdown("**’Ñ’°’∂÷Ä’°’¥’°’Ω’∂ ’®’Ω’ø ’∞’°÷Ä÷Å’•÷Ä’´ (’æ’´’≥’°’Ø / ’Ø’∑’´’º / ’∂’•÷Ä’§÷Ä’∏÷Ç’¥):**")
            st.dataframe(dbg_rows, use_container_width=True)

            st.markdown("**’ç’•’Ø÷Å’´’°’∂’•÷Ä’´ ’°’¥÷É’∏÷É’∏÷Ç’¥ (’Ω’ø’∏÷Ç’£’∏÷Ç’¥ ’Ø’∑’´’º’∂’•÷Ä’´ ’∞’°’¥’°’∫’°’ø’°’Ω’≠’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’®):**")
            st.dataframe(dbg_sum, use_container_width=True)

            scen_calc = dbg_rows["scenario_calc_pct"].dropna().unique()
            if scen_calc.size:
                st.info(f"’ç÷Å’•’∂’°÷Ä’´ ’æ’•÷Ä’°’∞’°’∑’æ’°÷Ä’Ø’æ’°’Æ ’ø’∏’Ø’∏’Ω’®: {scen_calc[0]:.2f}%")

# --- NEW: ‘±’∏÷Ç’§’´’∏ ---
with tab_audio:
    st.subheader("‘±’∏÷Ç’§’´’∏ ÷Ü’°’µ’¨’•÷Ä")
    st.caption("‘≤’•’º’∂’•÷Ñ, ’¨’Ω’•÷Ñ, ’≠’¥’¢’°’£÷Ä’•÷Ñ ’æ’•÷Ä’∂’°’£’´÷Ä’®/’∂’Ø’°÷Ä’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ÷á ’ª’∂’ª’•÷Ñ ’®’Ω’ø ’°’∂’∞÷Ä’°’™’•’∑’ø’∏÷Ç’©’µ’°’∂÷â")
    sb, BUCKET = _sb_client()
    if sb is None:
        st.stop()
    render_audio_tab(sb, BUCKET)

st.caption("¬© ‘¥’°’∑’¢’∏÷Ä’§ ‚Äî ’∞’°’∑’æ’°÷Ä’Ø’® ’∞’´’¥’∂’æ’°’Æ ’ß ¬´‘±’µ’∏¬ª ’∫’°’ø’°’Ω’≠’°’∂’∂’•÷Ä’´ ’Ø’∑’º’°’Æ ’¢’°’™’∂’´’∂÷â LAS/YAP ’Ø’∑’´’º’∂’•÷Ä’® ’Ø’´÷Ä’°’º’æ’∏÷Ç’¥ ’•’∂ ’¢’°’™’∂’´ (E) ÷á ’Ω÷Å’•’∂’°÷Ä’´ (F) ’¥’°’Ø’°÷Ä’§’°’Ø’∂’•÷Ä’∏÷Ç’¥÷â")
